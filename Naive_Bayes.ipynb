{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e170df2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T14:11:38.514466Z",
     "iopub.status.busy": "2025-04-21T14:11:38.514006Z",
     "iopub.status.idle": "2025-04-21T14:11:45.973666Z",
     "shell.execute_reply": "2025-04-21T14:11:45.972718Z",
     "shell.execute_reply.started": "2025-04-21T14:11:38.514432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyvi import ViTokenizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9d511fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T14:11:45.976365Z",
     "iopub.status.busy": "2025-04-21T14:11:45.975814Z",
     "iopub.status.idle": "2025-04-21T14:11:46.765262Z",
     "shell.execute_reply": "2025-04-21T14:11:46.764161Z",
     "shell.execute_reply.started": "2025-04-21T14:11:45.976338Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'./cleaned_data/train.csv', quotechar='\"', encoding='utf-8')\n",
    "valid_df = pd.read_csv(r'./cleaned_data/valid.csv', quotechar='\"', encoding='utf-8')\n",
    "test_df = pd.read_csv(r'./cleaned_data/test.csv', quotechar='\"', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa15d137",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T14:11:46.766429Z",
     "iopub.status.busy": "2025-04-21T14:11:46.766070Z",
     "iopub.status.idle": "2025-04-21T14:11:46.780031Z",
     "shell.execute_reply": "2025-04-21T14:11:46.778953Z",
     "shell.execute_reply.started": "2025-04-21T14:11:46.766396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([train_df, valid_df, test_df], ignore_index=True)\n",
    "sentences = df['sentence'].values\n",
    "labels = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e0838b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T14:11:46.782973Z",
     "iopub.status.busy": "2025-04-21T14:11:46.782595Z",
     "iopub.status.idle": "2025-04-21T14:11:46.799367Z",
     "shell.execute_reply": "2025-04-21T14:11:46.798168Z",
     "shell.execute_reply.started": "2025-04-21T14:11:46.782947Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_vietnamese(text):\n",
    "    return ' '.join(ViTokenizer.tokenize(text).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b81ddc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T14:11:46.801296Z",
     "iopub.status.busy": "2025-04-21T14:11:46.800665Z",
     "iopub.status.idle": "2025-04-21T14:13:19.273312Z",
     "shell.execute_reply": "2025-04-21T14:13:19.272082Z",
     "shell.execute_reply.started": "2025-04-21T14:11:46.801248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tokenized_sentences = [tokenize_vietnamese(sentence) for sentence in sentences]\n",
    "count_vectorizer = CountVectorizer(max_features=500)\n",
    "X = count_vectorizer.fit_transform(tokenized_sentences).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba07cf60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T14:13:19.274690Z",
     "iopub.status.busy": "2025-04-21T14:13:19.274363Z",
     "iopub.status.idle": "2025-04-21T14:13:19.369390Z",
     "shell.execute_reply": "2025-04-21T14:13:19.368197Z",
     "shell.execute_reply.started": "2025-04-21T14:13:19.274661Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f66ef1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes Test Accuracy: 0.8251311012505043\n",
      "\n",
      "Naive Bayes Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.78      0.82      4993\n",
      "           1       0.79      0.88      0.83      4923\n",
      "\n",
      "    accuracy                           0.83      9916\n",
      "   macro avg       0.83      0.83      0.82      9916\n",
      "weighted avg       0.83      0.83      0.82      9916\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_model = MultinomialNB(alpha=1, fit_prior=False)\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "print(\"\\nNaive Bayes Test Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"\\nNaive Bayes Classification Report:\\n\", classification_report(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3e093c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T14:20:41.297651Z",
     "iopub.status.busy": "2025-04-21T14:20:41.297383Z",
     "iopub.status.idle": "2025-04-21T14:20:41.313756Z",
     "shell.execute_reply": "2025-04-21T14:20:41.312319Z",
     "shell.execute_reply.started": "2025-04-21T14:20:41.297628Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment of new sentence: 1\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(sentence, vectorizer, model):\n",
    "    tokenized = tokenize_vietnamese(sentence)\n",
    "    vector = vectorizer.transform([tokenized]).toarray()\n",
    "    prediction = model.predict(vector)\n",
    "    return prediction[0]\n",
    "\n",
    "new_sentence = \"Đồ ăn đóng gói cẩn thận nhưng không ngon lắm.\"\n",
    "print(\"\\nSentiment of new sentence:\", predict_sentiment(new_sentence, count_vectorizer, nb_model))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7209281,
     "sourceId": 11499699,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
