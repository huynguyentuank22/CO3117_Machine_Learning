{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80ce350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c0eb820",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"5CD-AI/Vietnamese-Sentiment-visobert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"5CD-AI/Vietnamese-Sentiment-visobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01976147",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./cleaned_data/train.csv')\n",
    "valid_df = pd.read_csv('./cleaned_data/valid.csv')\n",
    "test_df = pd.read_csv('./cleaned_data/test.csv')\n",
    "# Làm sạch dữ liệu\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    emoji_pattern = re.compile(\":[a-zA-Z0-9_]+:\")\n",
    "    text = emoji_pattern.sub(\"\", text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'[\\s\\_]+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "train_df['sentence'] = train_df['sentence'].apply(clean_text)\n",
    "valid_df['sentence'] = valid_df['sentence'].apply(clean_text)\n",
    "test_df['sentence'] = test_df['sentence'].apply(clean_text)\n",
    "# dataset = [train_df, valid_df, test_df]\n",
    "# for data in dataset:\n",
    "#     for i, text in enumerate(data['sentence'].tolist()):\n",
    "#         try:\n",
    "#             encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "#             max_id = encoding['input_ids'].max().item()\n",
    "#             if max_id >= 64001:\n",
    "#                 print(f\"Câu lỗi tại index {i}: {text}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Lỗi tại index {i}: {text}, Chi tiết: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34cb7996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mua có mỗi bingsu thập cẩm 45k mà mình f đợi h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thứ 6 nào ta cùng quẩy vuvuzela beer club chun...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mình đi với nhóm tổng cộng 4 người ăn chỉ có k...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nhân viên phục vụ không mấy tận tình đồ ăn ra ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vào đây thì hết bàn nhưng mình vẫn ngồi đợi bì...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29732</th>\n",
       "      <td>29 mình đi với nhóm bạn tổng cộng là 8ngthiệt ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29733</th>\n",
       "      <td>sushi bình dân mà chất lượng không bình dân ch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29734</th>\n",
       "      <td>trời ơi từ bé đến lớn chưa thử món kem nào bằn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29735</th>\n",
       "      <td>nge mn cũng ns ngon nên hni đến coi thế nào qu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29736</th>\n",
       "      <td>ks đẹp thoág mát lại gần vs phố cổ nữa nên rất...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29737 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label\n",
       "0      mua có mỗi bingsu thập cẩm 45k mà mình f đợi h...      0\n",
       "1      thứ 6 nào ta cùng quẩy vuvuzela beer club chun...      0\n",
       "2      mình đi với nhóm tổng cộng 4 người ăn chỉ có k...      0\n",
       "3      nhân viên phục vụ không mấy tận tình đồ ăn ra ...      0\n",
       "4      vào đây thì hết bàn nhưng mình vẫn ngồi đợi bì...      0\n",
       "...                                                  ...    ...\n",
       "29732  29 mình đi với nhóm bạn tổng cộng là 8ngthiệt ...      1\n",
       "29733  sushi bình dân mà chất lượng không bình dân ch...      1\n",
       "29734  trời ơi từ bé đến lớn chưa thử món kem nào bằn...      1\n",
       "29735  nge mn cũng ns ngon nên hni đến coi thế nào qu...      1\n",
       "29736  ks đẹp thoág mát lại gần vs phố cổ nữa nên rất...      1\n",
       "\n",
       "[29737 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e11e9fd",
   "metadata": {},
   "source": [
    "# **Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0bb2b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "\n",
    "# Áp dụng tokenize cho từng tập dữ liệu\n",
    "train_encodings = tokenize_function(train_df['sentence'].tolist())\n",
    "valid_encodings = tokenize_function(valid_df['sentence'].tolist())\n",
    "test_encodings = tokenize_function(test_df['sentence'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ac219c",
   "metadata": {},
   "source": [
    "# **Prepare dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cda89da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c16af337",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentimentDataset(train_encodings, train_df['label'].tolist())\n",
    "valid_dataset = SentimentDataset(valid_encodings, valid_df['label'].tolist())\n",
    "test_dataset = SentimentDataset(test_encodings, test_df['label'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be9db0",
   "metadata": {},
   "source": [
    "# **Config train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9c9fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ed8bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * 3)  # 3 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad919b3",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f39ecf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(15004, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7239380e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training: 100%|██████████| 1859/1859 [08:08<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.2404482413284709\n",
      "Epoch 1, Valid Loss: 0.23040283209120857, Accuracy: 0.9114000604777744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training: 100%|██████████| 1859/1859 [08:09<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.16454656555774738\n",
      "Epoch 2, Valid Loss: 0.22777276060389484, Accuracy: 0.9184558008265296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training: 100%|██████████| 1859/1859 [08:10<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.0997015593318953\n",
      "Epoch 3, Valid Loss: 0.272244840523681, Accuracy: 0.9110976716056849\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Huấn luyện\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\")):\n",
    "#         optimizer.zero_grad()\n",
    "#         inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "#         labels = batch['labels'].to(device)\n",
    "#         # Kiểm tra input_ids\n",
    "#         max_id = inputs['input_ids'].max().item()\n",
    "#         if max_id >= 64001:\n",
    "#             print(f\"Lỗi trong batch {i}: max input_id = {max_id}\")\n",
    "#             print(f\"input_ids: {inputs['input_ids']}\")\n",
    "#             raise ValueError(\"Tìm thấy input_id không hợp lệ\")\n",
    "#         if 'position_ids' in inputs:\n",
    "#             max_pos_id = inputs['position_ids'].max().item()\n",
    "#             if max_pos_id >= 512:  # PhoBERT max_position_embeddings thường là 512\n",
    "#                 print(f\"Lỗi trong batch {i}: max position_id = {max_pos_id}\")\n",
    "#                 print(f\"position_ids: {inputs['position_ids']}\")\n",
    "#                 raise ValueError(\"Tìm thấy position_id không hợp lệ\")\n",
    "#         outputs = model(**inputs, labels=labels)\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "#         train_loss += loss.item()\n",
    "#     print(f\"Epoch {epoch+1}, Train Loss: {train_loss / len(train_loader)}\")\n",
    "    \n",
    "    # Đánh giá trên tập valid\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            valid_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}, Valid Loss: {valid_loss / len(valid_loader)}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ead04",
   "metadata": {},
   "source": [
    "# **Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04a58628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9110976716056849\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(**inputs)\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2814894a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ticklab/source/BaoTram/CO3117_Machine_Learning/env/lib/python3.12/site-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 256}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./modelTransformer/tokenizer_config.json',\n",
       " './modelTransformer/special_tokens_map.json',\n",
       " './modelTransformer/tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./modelTransformer/\")\n",
    "tokenizer.save_pretrained(\"./modelTransformer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44dd164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"./modelTransformer/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./modelTransformer/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
